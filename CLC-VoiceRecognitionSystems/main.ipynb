{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import py7zr\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and extraction directories\n",
    "data_dir = \"/kaggle/input/tensorflow-speech-recognition-challenge/\"\n",
    "extract_dir = '/kaggle/working/extracted_data_train'\n",
    "\n",
    "def extract_7z(filepath, dest_dir):\n",
    "    with py7zr.SevenZipFile(filepath, mode='r') as z:\n",
    "        z.extractall(path=dest_dir)\n",
    "\n",
    "# Extract the data if it hasn't been already\n",
    "if not os.path.exists(extract_dir):\n",
    "    filepath = os.path.join(data_dir, \"train.7z\")\n",
    "    print(f\"Extracting files from {filepath} to {extract_dir}...\")\n",
    "    extract_7z(filepath, extract_dir)\n",
    "else:\n",
    "    print(f\"Data already extracted at {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load command labels\n",
    "commands = np.array([d for d in tf.io.gfile.listdir(os.path.join(extract_dir, \"train/audio\")) \n",
    "                    if d != '_background_noise_'])\n",
    "print('Commands:', commands)\n",
    "\n",
    "# Load background noise files for data augmentation\n",
    "noise_dir = os.path.join(extract_dir, \"train/audio/_background_noise_\")\n",
    "noise_files = tf.io.gfile.glob(noise_dir + '/*.wav')\n",
    "\n",
    "# Function to decode audio files\n",
    "def decode_audio(audio_binary):\n",
    "    audio, _ = tf.audio.decode_wav(contents=audio_binary)\n",
    "    return tf.squeeze(audio, axis=-1)\n",
    "\n",
    "# Function to extract label from file path\n",
    "def get_label(file_path):\n",
    "    label = tf.strings.split(input=file_path, sep=os.path.sep)[-2]\n",
    "    return tf.cond(tf.reduce_any(tf.equal(commands, label)), lambda: label, lambda: tf.constant(\"unknown\", dtype=tf.string))\n",
    "\n",
    "# Function to convert waveform to spectrogram\n",
    "def get_spectrogram(waveform):\n",
    "    input_len = 16000\n",
    "    waveform = waveform[:input_len]\n",
    "    # Pad with zeros if audio is shorter than 16000 samples\n",
    "    zero_padding = tf.zeros([input_len] - tf.shape(waveform), dtype=tf.float32)\n",
    "    waveform = tf.cast(waveform, dtype=tf.float32)\n",
    "    equal_length = tf.concat([waveform, zero_padding], 0)\n",
    "    \n",
    "    spectrogram = tf.signal.stft(equal_length, frame_length=255, frame_step=128)\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    return spectrogram[..., tf.newaxis]\n",
    "\n",
    "# Function to add background noise for data augmentation\n",
    "def add_background_noise(waveform, noise_files):\n",
    "    noise_file = random.choice(noise_files)\n",
    "    noise_audio_binary = tf.io.read_file(noise_file)\n",
    "    noise_waveform = decode_audio(noise_audio_binary)\n",
    "\n",
    "    # Adjust noise length to match the audio\n",
    "    waveform_len = tf.shape(waveform)[0]\n",
    "    noise_len = tf.shape(noise_waveform)[0]\n",
    "    if noise_len > waveform_len:\n",
    "        offset = tf.random.uniform(shape=[], minval=0, maxval=noise_len - waveform_len, dtype=tf.int32)\n",
    "        noise_waveform = noise_waveform[offset:offset + waveform_len]\n",
    "    else:\n",
    "        padding = tf.zeros([waveform_len - noise_len], dtype=tf.float32)\n",
    "        noise_waveform = tf.concat([noise_waveform, padding], axis=0)\n",
    "\n",
    "    noise_factor = tf.random.uniform(shape=[], minval=0.0, maxval=0.5)\n",
    "    augmented_waveform = waveform + noise_factor * noise_waveform\n",
    "    return tf.clip_by_value(augmented_waveform, -1.0, 1.0)\n",
    "\n",
    "# Function to load and preprocess audio data\n",
    "def preprocess_dataset(files, augment=False):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    output_ds = files_ds.map(\n",
    "        lambda file_path: (tf.io.read_file(file_path), get_label(file_path)), \n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    output_ds = output_ds.map(\n",
    "        lambda audio_binary, label: (decode_audio(audio_binary), label),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    if augment:\n",
    "        output_ds = output_ds.map(\n",
    "            lambda waveform, label: (add_background_noise(waveform, noise_files), label), \n",
    "            num_parallel_calls=tf.data.AUTOTUNE\n",
    "        )\n",
    "    output_ds = output_ds.map(\n",
    "        lambda waveform, label: (get_spectrogram(waveform), tf.argmax(label == commands)),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    return output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_data = os.path.join(extract_dir, \"train/audio\")\n",
    "\n",
    "# Get all filenames, excluding noise files\n",
    "filenames = tf.io.gfile.glob([os.path.join(filepath_data, d,  '*') for d in commands])\n",
    "filenames = tf.random.shuffle(filenames)\n",
    "\n",
    "total_samples = len(filenames)\n",
    "train_size = int(0.8 * total_samples)  # 80% for training\n",
    "val_size = int(0.1 * total_samples)  # 10% for validation\n",
    "test_size = total_samples - train_size - val_size  # Remaining 10% for testing\n",
    "\n",
    "# Split dataset into train, validation, and test sets\n",
    "train_files = filenames[:train_size]\n",
    "val_files = filenames[train_size:train_size + val_size]\n",
    "test_files = filenames[train_size + val_size:]\n",
    "\n",
    "print('Training set size:', len(train_files))\n",
    "print('Validation set size:', len(val_files))\n",
    "print('Test set size:', len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = preprocess_dataset(train_files, augment=True)\n",
    "val_ds = preprocess_dataset(val_files)\n",
    "test_ds = preprocess_dataset(test_files)\n",
    "\n",
    "# Batch the datasets for training\n",
    "batch_size = 32\n",
    "train_ds = train_ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input shape from the data\n",
    "for spectrogram, _ in train_ds.take(1):\n",
    "    input_shape = spectrogram.shape[1:]\n",
    "\n",
    "print('Input shape:', input_shape)\n",
    "num_labels = len(commands)\n",
    "\n",
    "# Normalize input\n",
    "norm_layer = layers.Normalization()\n",
    "norm_layer.adapt(data=train_ds.map(lambda spec, label: spec))\n",
    "\n",
    "timesteps = 16\n",
    "\n",
    "# Build the model\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Resizing(32, 32),\n",
    "    norm_layer,\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Reshape((-1, timesteps, 21632 // timesteps)), \n",
    "    tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=1)),\n",
    "    layers.GRU(64),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = history.history\n",
    "plt.plot(history.epoch, metrics['loss'], metrics['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()\n",
    "\n",
    "test_audio = []\n",
    "test_labels = []\n",
    "\n",
    "for audio, label in test_ds:\n",
    "    test_audio.append(audio.numpy())\n",
    "    test_labels.append(label.numpy())\n",
    "\n",
    "test_audio = np.array(test_audio)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "y_pred = np.argmax(model.predict(test_audio), axis=1)\n",
    "y_true = test_labels\n",
    "\n",
    "test_acc = sum(y_pred == y_true) / len(y_true)\n",
    "print(f'Test set accuracy: {test_acc:.0%}')\n",
    "\n",
    "confusion_mtx = tf.math.confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = \"/kaggle/working/extracted_data/train/audio/down/0a9f9af7_nohash_0.wav\"\n",
    "sample_ds = preprocess_dataset([str(sample_file)])\n",
    "\n",
    "for spectrogram, label in sample_ds.batch(1):\n",
    "    prediction = model(spectrogram)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.bar(commands, tf.nn.softmax(prediction[0]))\n",
    "    plt.title(f'Predictions for \"{commands[label[0]]}\"')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_sample = pd.read_csv(\"/kaggle/working/sample_submission.csv\")\n",
    "submission_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and extraction directories\n",
    "data_dir = \"/kaggle/input/tensorflow-speech-recognition-challenge/\"\n",
    "extract_dir = '/kaggle/working/extracted_data_test/'\n",
    "\n",
    "# Extract the data if it hasn't been already\n",
    "if not os.path.exists(extract_dir):\n",
    "    filepath = os.path.join(data_dir, \"test.7z\")\n",
    "    print(f\"Extracting files from {filepath} to {extract_dir}...\")\n",
    "    extract_7z(filepath, extract_dir)\n",
    "else:\n",
    "    print(f\"Data already extracted at {extract_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_data = \"/kaggle/working/extracted_data_test/test/audio/\"\n",
    "filenames = os.listdir(filepath_data)\n",
    "filenames_path = [filepath_data + i for i in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = preprocess_dataset(filenames_path)\n",
    "test_ds = test_ds.batch(1)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for spectrogram in tqdm(test_ds):\n",
    "    spectrogram = spectrogram[0]\n",
    "    prediction = model(spectrogram)\n",
    "    \n",
    "    probabilities = tf.nn.softmax(prediction[0])\n",
    "    predicted_index = tf.argmax(probabilities).numpy()\n",
    "    predicted_command = commands[predicted_index]\n",
    "    \n",
    "    labels.append(predicted_command)\n",
    "\n",
    "#     print(f\"Probabilities: {probabilities.numpy()}\")\n",
    "#     print(f\"Predicted Command: {predicted_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"fnane\":filenames,\"label\":labels})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
