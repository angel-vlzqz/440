{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def preprocess_obesity_data(file_path):\n",
    "    # 1. Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    # 3. Simple data validation\n",
    "    # Remove any rows where height or weight are unreasonable\n",
    "    df = df[(df['Height'] > 1.4) & (df['Height'] < 2.2) &\n",
    "            (df['Weight'] > 40) & (df['Weight'] < 200)]\n",
    "    \n",
    "    # 4. Encode categorical variables using Label Encoder\n",
    "    categorical_cols = ['Gender', 'family_history', 'FAVC', 'CAEC', \n",
    "                       'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    # 5. Scale numerical variables (now without Weight)\n",
    "    numerical_cols = ['Age', 'Height', 'FCVC', 'NCP', \n",
    "                     'CH2O', 'FAF', 'TUE']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 6. Split features and target, dropping both BMI and Weight\n",
    "    X = df.drop(['BMI', 'Weight'], axis=1)\n",
    "    y = df['BMI']\n",
    "    \n",
    "    # 7. Print basic statistics\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nFeature names:\", list(X.columns))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Gender            0\n",
      "Age               0\n",
      "Height            0\n",
      "Weight            0\n",
      "family_history    0\n",
      "FAVC              0\n",
      "FCVC              0\n",
      "NCP               0\n",
      "CAEC              0\n",
      "SMOKE             0\n",
      "CH2O              0\n",
      "SCC               0\n",
      "FAF               0\n",
      "TUE               0\n",
      "CALC              0\n",
      "MTRANS            0\n",
      "BMI               0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (2105, 17)\n",
      "\n",
      "Feature names: ['Gender', 'Age', 'Height', 'family_history', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']\n"
     ]
    }
   ],
   "source": [
    "x,y =preprocess_obesity_data(\"ObesityPrediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       24.386526\n",
      "1       24.238227\n",
      "2       23.765432\n",
      "3       26.851852\n",
      "4       28.342381\n",
      "          ...    \n",
      "2106    44.901475\n",
      "2107    43.741923\n",
      "2108    43.543817\n",
      "2109    44.071535\n",
      "2110    44.144338\n",
      "Name: BMI, Length: 2105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Gender       Age    Height  family_history  FAVC      FCVC       NCP  \\\n",
      "0          0 -0.524137 -0.882009               1     0 -0.793282  0.402928   \n",
      "1          0 -0.524137 -1.956374               1     0  1.087496  0.402928   \n",
      "2          1 -0.209058  1.051847               1     0 -0.793282  0.402928   \n",
      "3          1  0.421101  1.051847               0     0  1.087496  0.402928   \n",
      "4          1 -0.366598  0.836974               0     0 -0.793282 -2.168920   \n",
      "...      ...       ...       ...             ...   ...       ...       ...   \n",
      "2106       0 -0.527786  0.092762               1     1  1.087496  0.402928   \n",
      "2107       0 -0.369285  0.499452               1     1  1.087496  0.402928   \n",
      "2108       0 -0.284041  0.538365               1     1  1.087496  0.402928   \n",
      "2109       0  0.005501  0.401319               1     1  1.087496  0.402928   \n",
      "2110       0 -0.104340  0.394723               1     1  1.087496  0.402928   \n",
      "\n",
      "      CAEC  SMOKE      CH2O  SCC       FAF       TUE  CALC  MTRANS  \n",
      "0        2      0 -0.014522    0 -1.187744  0.568638     3       3  \n",
      "1        2      1  1.616743    1  2.346544 -1.079475     2       3  \n",
      "2        2      0 -0.014522    0  1.168448  0.568638     1       3  \n",
      "3        2      0 -0.014522    0  1.168448 -1.079475     1       4  \n",
      "4        2      0 -0.014522    0 -1.187744 -1.079475     2       3  \n",
      "...    ...    ...       ...  ...       ...       ...   ...     ...  \n",
      "2106     2      0 -0.458000    0  0.787062  0.414123     2       3  \n",
      "2107     2      0 -0.006154    0  0.392542 -0.091810     2       3  \n",
      "2108     2      0  0.073881    0  0.478330 -0.014319     2       3  \n",
      "2109     2      0  1.375869    0  0.154233 -0.113623     2       3  \n",
      "2110     2      0  1.394097    0  0.021515  0.097504     2       3  \n",
      "\n",
      "[2105 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/150], Train Loss: 25.9066, Val Loss: 26.1104\n",
      "Epoch [20/150], Train Loss: 18.0231, Val Loss: 17.1914\n",
      "Epoch [30/150], Train Loss: 6.6170, Val Loss: 5.6118\n",
      "Epoch [40/150], Train Loss: 2.0106, Val Loss: 1.6657\n",
      "Epoch [50/150], Train Loss: 1.8064, Val Loss: 1.5870\n",
      "Epoch [60/150], Train Loss: 1.5834, Val Loss: 1.4270\n",
      "Epoch [70/150], Train Loss: 1.6030, Val Loss: 1.3447\n",
      "Epoch [80/150], Train Loss: 1.4507, Val Loss: 1.3036\n",
      "Epoch [90/150], Train Loss: 1.5964, Val Loss: 1.3127\n",
      "Epoch [100/150], Train Loss: 1.4722, Val Loss: 1.2765\n",
      "Epoch [110/150], Train Loss: 1.4465, Val Loss: 1.2745\n",
      "Epoch [120/150], Train Loss: 1.4271, Val Loss: 1.2563\n",
      "Epoch [130/150], Train Loss: 1.3831, Val Loss: 1.2528\n",
      "Epoch [140/150], Train Loss: 1.4460, Val Loss: 1.2625\n",
      "Epoch [150/150], Train Loss: 1.3791, Val Loss: 1.2426\n",
      "\n",
      "Training Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.8587\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.7387\n",
      "\n",
      "Detailed Error Analysis (Test Set):\n",
      "Mean Absolute Error: 1.67 BMI points\n",
      "Median Absolute Error: 0.85 BMI points\n",
      "90th percentile of absolute error: 4.49 BMI points\n",
      "Percentage of predictions within 2.0 BMI points: 73.9%\n",
      "\n",
      "Error Distribution:\n",
      "Error 0-1 BMI points: 55.3%\n",
      "Error 1-2 BMI points: 18.5%\n",
      "Error 2-3 BMI points: 10.0%\n",
      "Error 3-4 BMI points: 4.3%\n",
      "Error 4-5 BMI points: 3.8%\n",
      "Error 5+ BMI points: 8.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_7144\\343777933.py:187: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BMIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class EnhancedBMINet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(EnhancedBMINet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Parallel branches for different feature scales\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Linear(128, 32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(32)\n",
    "        )\n",
    "        \n",
    "        # Concatenated features processing\n",
    "        self.combined = nn.Sequential(\n",
    "            nn.Linear(96, 48),  # 64 + 32 = 96\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            nn.Linear(48, 24),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(24),\n",
    "            \n",
    "            nn.Linear(24, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Extract features\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Process through parallel branches\n",
    "        branch1_out = self.branch1(features)\n",
    "        branch2_out = self.branch2(features)\n",
    "        \n",
    "        # Concatenate branch outputs\n",
    "        combined = torch.cat((branch1_out, branch2_out), dim=1)\n",
    "        \n",
    "        # Final processing\n",
    "        return self.combined(combined)\n",
    "\n",
    "def train_and_evaluate_bmi_model(X, y, epochs=150, batch_size=64, threshold=2.0):\n",
    "    # Split the data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = BMIDataset(X_train, y_train)\n",
    "    test_dataset = BMIDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = EnhancedBMINet(X.shape[1])\n",
    "    criterion = nn.HuberLoss(delta=1.0)  # Combines MSE and MAE, more robust to outliers\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 15\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y.unsqueeze(1)).item()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping with best model saving\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(test_loader):.4f}')\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get all predictions\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            train_predictions.extend(outputs.numpy().flatten())\n",
    "            train_true.extend(batch_y.numpy().flatten())\n",
    "            \n",
    "        test_predictions = []\n",
    "        test_true = []\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            test_predictions.extend(outputs.numpy().flatten())\n",
    "            test_true.extend(batch_y.numpy().flatten())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    train_true = np.array(train_true)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_true = np.array(test_true)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    train_within_threshold = np.abs(train_true - train_predictions) <= threshold\n",
    "    test_within_threshold = np.abs(test_true - test_predictions) <= threshold\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {np.mean(train_within_threshold):.4f}\")\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {np.mean(test_within_threshold):.4f}\")\n",
    "    \n",
    "    # Error analysis\n",
    "    errors = np.abs(test_true - test_predictions)\n",
    "    print(\"\\nDetailed Error Analysis (Test Set):\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(errors):.2f} BMI points\")\n",
    "    print(f\"Median Absolute Error: {np.median(errors):.2f} BMI points\")\n",
    "    print(f\"90th percentile of absolute error: {np.percentile(errors, 90):.2f} BMI points\")\n",
    "    print(f\"Percentage of predictions within {threshold} BMI points: {(100 * np.mean(test_within_threshold)):.1f}%\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_bins = pd.cut(errors, bins=[0, 1, 2, 3, 4, 5, float('inf')], \n",
    "                       labels=['0-1', '1-2', '2-3', '3-4', '4-5', '5+'])\n",
    "    error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n",
    "    print(\"\\nError Distribution:\")\n",
    "    for bin_name, percentage in error_distribution.items():\n",
    "        print(f\"Error {bin_name} BMI points: {percentage*100:.1f}%\")\n",
    "    \n",
    "    return model, errors\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming X and y are already preprocessed using your preprocess_obesity_data function\n",
    "    \n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model, errors = train_and_evaluate_bmi_model(x, y, threshold=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
