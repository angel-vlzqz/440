{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def preprocess_obesity_data(file_path):\n",
    "    # 1. Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    # 3. Simple data validation\n",
    "    # Remove any rows where height or weight are unreasonable\n",
    "    df = df[(df['Height'] > 1.4) & (df['Height'] < 2.2) &\n",
    "            (df['Weight'] > 40) & (df['Weight'] < 200)]\n",
    "    \n",
    "    # 4. Encode categorical variables using Label Encoder\n",
    "    categorical_cols = ['Gender', 'family_history', 'FAVC', 'CAEC', \n",
    "                       'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    # 5. Scale numerical variables (now without Weight)\n",
    "    numerical_cols = ['Age', 'Height', 'FCVC', 'NCP', \n",
    "                     'CH2O', 'FAF', 'TUE']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 6. Split features and target, dropping both BMI and Weight\n",
    "    X = df.drop(['BMI', 'Weight'], axis=1)\n",
    "    y = df['BMI']\n",
    "    \n",
    "    # 7. Print basic statistics\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nFeature names:\", list(X.columns))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Gender            0\n",
      "Age               0\n",
      "Height            0\n",
      "Weight            0\n",
      "family_history    0\n",
      "FAVC              0\n",
      "FCVC              0\n",
      "NCP               0\n",
      "CAEC              0\n",
      "SMOKE             0\n",
      "CH2O              0\n",
      "SCC               0\n",
      "FAF               0\n",
      "TUE               0\n",
      "CALC              0\n",
      "MTRANS            0\n",
      "BMI               0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (2105, 17)\n",
      "\n",
      "Feature names: ['Gender', 'Age', 'Height', 'family_history', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']\n"
     ]
    }
   ],
   "source": [
    "x,y =preprocess_obesity_data(\"ObesityPrediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       24.386526\n",
      "1       24.238227\n",
      "2       23.765432\n",
      "3       26.851852\n",
      "4       28.342381\n",
      "          ...    \n",
      "2106    44.901475\n",
      "2107    43.741923\n",
      "2108    43.543817\n",
      "2109    44.071535\n",
      "2110    44.144338\n",
      "Name: BMI, Length: 2105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Gender       Age    Height  family_history  FAVC      FCVC       NCP  \\\n",
      "0          0 -0.524137 -0.882009               1     0 -0.793282  0.402928   \n",
      "1          0 -0.524137 -1.956374               1     0  1.087496  0.402928   \n",
      "2          1 -0.209058  1.051847               1     0 -0.793282  0.402928   \n",
      "3          1  0.421101  1.051847               0     0  1.087496  0.402928   \n",
      "4          1 -0.366598  0.836974               0     0 -0.793282 -2.168920   \n",
      "...      ...       ...       ...             ...   ...       ...       ...   \n",
      "2106       0 -0.527786  0.092762               1     1  1.087496  0.402928   \n",
      "2107       0 -0.369285  0.499452               1     1  1.087496  0.402928   \n",
      "2108       0 -0.284041  0.538365               1     1  1.087496  0.402928   \n",
      "2109       0  0.005501  0.401319               1     1  1.087496  0.402928   \n",
      "2110       0 -0.104340  0.394723               1     1  1.087496  0.402928   \n",
      "\n",
      "      CAEC  SMOKE      CH2O  SCC       FAF       TUE  CALC  MTRANS  \n",
      "0        2      0 -0.014522    0 -1.187744  0.568638     3       3  \n",
      "1        2      1  1.616743    1  2.346544 -1.079475     2       3  \n",
      "2        2      0 -0.014522    0  1.168448  0.568638     1       3  \n",
      "3        2      0 -0.014522    0  1.168448 -1.079475     1       4  \n",
      "4        2      0 -0.014522    0 -1.187744 -1.079475     2       3  \n",
      "...    ...    ...       ...  ...       ...       ...   ...     ...  \n",
      "2106     2      0 -0.458000    0  0.787062  0.414123     2       3  \n",
      "2107     2      0 -0.006154    0  0.392542 -0.091810     2       3  \n",
      "2108     2      0  0.073881    0  0.478330 -0.014319     2       3  \n",
      "2109     2      0  1.375869    0  0.154233 -0.113623     2       3  \n",
      "2110     2      0  1.394097    0  0.021515  0.097504     2       3  \n",
      "\n",
      "[2105 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/200], Train Loss: 26.3244, Val Loss: 26.5694\n",
      "Epoch [40/200], Train Loss: 19.2669, Val Loss: 19.2073\n",
      "Epoch [60/200], Train Loss: 8.9139, Val Loss: 8.6395\n",
      "Epoch [80/200], Train Loss: 2.4676, Val Loss: 1.8835\n",
      "Epoch [100/200], Train Loss: 1.9709, Val Loss: 1.7956\n",
      "Epoch [120/200], Train Loss: 1.8658, Val Loss: 1.6052\n",
      "Epoch [140/200], Train Loss: 1.7139, Val Loss: 1.4959\n",
      "Early stopping at epoch 155\n",
      "\n",
      "Training Model 2/5\n",
      "Epoch [20/200], Train Loss: 26.5179, Val Loss: 26.0841\n",
      "Epoch [40/200], Train Loss: 19.5062, Val Loss: 18.7551\n",
      "Epoch [60/200], Train Loss: 8.8219, Val Loss: 8.5327\n",
      "Epoch [80/200], Train Loss: 2.4452, Val Loss: 2.0320\n",
      "Epoch [100/200], Train Loss: 1.9348, Val Loss: 1.6039\n",
      "Epoch [120/200], Train Loss: 1.6685, Val Loss: 1.5545\n",
      "Epoch [140/200], Train Loss: 1.5275, Val Loss: 1.4455\n",
      "Epoch [160/200], Train Loss: 1.5604, Val Loss: 1.4421\n",
      "Early stopping at epoch 167\n",
      "\n",
      "Training Model 3/5\n",
      "Epoch [20/200], Train Loss: 26.4208, Val Loss: 26.4255\n",
      "Epoch [40/200], Train Loss: 19.3265, Val Loss: 19.1859\n",
      "Epoch [60/200], Train Loss: 8.8222, Val Loss: 8.7602\n",
      "Epoch [80/200], Train Loss: 2.5599, Val Loss: 2.1424\n",
      "Epoch [100/200], Train Loss: 1.8673, Val Loss: 1.8167\n",
      "Epoch [120/200], Train Loss: 1.7581, Val Loss: 1.7114\n",
      "Early stopping at epoch 123\n",
      "\n",
      "Training Model 4/5\n",
      "Epoch [20/200], Train Loss: 26.4612, Val Loss: 26.2200\n",
      "Epoch [40/200], Train Loss: 19.3584, Val Loss: 19.6338\n",
      "Epoch [60/200], Train Loss: 8.8149, Val Loss: 7.5885\n",
      "Epoch [80/200], Train Loss: 2.4104, Val Loss: 1.8925\n",
      "Epoch [100/200], Train Loss: 2.0213, Val Loss: 1.6134\n",
      "Epoch [120/200], Train Loss: 1.6192, Val Loss: 1.4587\n",
      "Epoch [140/200], Train Loss: 1.5858, Val Loss: 1.4379\n",
      "Epoch [160/200], Train Loss: 1.5341, Val Loss: 1.4266\n",
      "Early stopping at epoch 171\n",
      "\n",
      "Training Model 5/5\n",
      "Epoch [20/200], Train Loss: 26.5319, Val Loss: 26.5466\n",
      "Epoch [40/200], Train Loss: 19.1828, Val Loss: 19.4534\n",
      "Epoch [60/200], Train Loss: 8.8297, Val Loss: 8.4825\n",
      "Epoch [80/200], Train Loss: 2.3064, Val Loss: 1.8783\n",
      "Epoch [100/200], Train Loss: 2.0283, Val Loss: 1.5750\n",
      "Epoch [120/200], Train Loss: 1.7731, Val Loss: 1.3422\n",
      "Epoch [140/200], Train Loss: 1.7073, Val Loss: 1.3326\n",
      "Epoch [160/200], Train Loss: 1.6992, Val Loss: 1.3140\n",
      "Epoch [180/200], Train Loss: 1.6610, Val Loss: 1.3254\n",
      "Early stopping at epoch 180\n",
      "\n",
      "Training Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.9341\n",
      "Mean Absolute Error: 0.75\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.7340\n",
      "Mean Absolute Error: 1.72\n",
      "\n",
      "Detailed Error Analysis (Test Set):\n",
      "Median Absolute Error: 0.88\n",
      "90th percentile of absolute error: 4.48\n",
      "\n",
      "Error Distribution:\n",
      "Error 0-1 BMI points: 54.6%\n",
      "Error 1-2 BMI points: 18.8%\n",
      "Error 2-3 BMI points: 7.8%\n",
      "Error 3-4 BMI points: 5.5%\n",
      "Error 4-5 BMI points: 5.0%\n",
      "Error 5+ BMI points: 8.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teddy\\AppData\\Local\\Temp\\ipykernel_7144\\3427618024.py:234: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class AdvancedBMINet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AdvancedBMINet, self).__init__()\n",
    "        \n",
    "        # Initial feature transformation\n",
    "        self.input_transform = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        # Deep feature extraction pathway\n",
    "        self.deep_pathway = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "        \n",
    "        # Wide pathway for direct feature processing\n",
    "        self.wide_pathway = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(128)\n",
    "        )\n",
    "        \n",
    "        # Expert subsystems\n",
    "        self.expert1 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        self.expert2 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        self.expert3 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(64)\n",
    "        )\n",
    "        \n",
    "        # Gating network for expert weighting\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(256, 3),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.GELU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial transformation\n",
    "        x = self.input_transform(x)\n",
    "        \n",
    "        # Process through pathways\n",
    "        deep_features = self.deep_pathway(x)\n",
    "        wide_features = self.wide_pathway(x)\n",
    "        \n",
    "        # Combine pathways\n",
    "        combined = torch.cat([deep_features, wide_features], dim=1)\n",
    "        \n",
    "        # Expert processing\n",
    "        expert1_out = self.expert1(combined)\n",
    "        expert2_out = self.expert2(combined)\n",
    "        expert3_out = self.expert3(combined)\n",
    "        \n",
    "        # Gate the experts\n",
    "        gates = self.gate(combined)\n",
    "        \n",
    "        # Combine expert outputs\n",
    "        expert_out = (gates[:, 0].unsqueeze(1) * expert1_out +\n",
    "                     gates[:, 1].unsqueeze(1) * expert2_out +\n",
    "                     gates[:, 2].unsqueeze(1) * expert3_out)\n",
    "        \n",
    "        # Final prediction\n",
    "        return self.final(expert_out)\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    def __init__(self, input_dim, n_models=5):\n",
    "        self.models = [AdvancedBMINet(input_dim) for _ in range(n_models)]\n",
    "        self.n_models = n_models\n",
    "        self.quantile_transformer = QuantileTransformer(output_distribution='normal')\n",
    "        \n",
    "    def preprocess_data(self, X, y=None, train=False):\n",
    "        if train:\n",
    "            X_processed = self.quantile_transformer.fit_transform(X)\n",
    "        else:\n",
    "            X_processed = self.quantile_transformer.transform(X)\n",
    "        return torch.FloatTensor(X_processed), None if y is None else torch.FloatTensor(y)\n",
    "    \n",
    "    def train(self, X, y, epochs=200, batch_size=128):\n",
    "        X_processed, y_processed = self.preprocess_data(X.values, y.values, train=True)\n",
    "        \n",
    "        # K-fold training for ensemble\n",
    "        kf = KFold(n_splits=self.n_models, shuffle=True, random_state=42)\n",
    "        \n",
    "        for model_idx, (train_idx, val_idx) in enumerate(kf.split(X_processed)):\n",
    "            print(f\"\\nTraining Model {model_idx + 1}/{self.n_models}\")\n",
    "            \n",
    "            # Prepare data\n",
    "            X_train, X_val = X_processed[train_idx], X_processed[val_idx]\n",
    "            y_train, y_val = y_processed[train_idx], y_processed[val_idx]\n",
    "            \n",
    "            train_dataset = TensorDataset(X_train, y_train.unsqueeze(1))\n",
    "            val_dataset = TensorDataset(X_val, y_val.unsqueeze(1))\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "            \n",
    "            # Initialize training components\n",
    "            model = self.models[model_idx]\n",
    "            criterion = nn.HuberLoss(delta=1.0)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                                factor=0.5, patience=5, verbose=True)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            patience = 20\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                train_loss = 0\n",
    "                for batch_X, batch_y in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                    # Optimizer step only (scheduler.step() moved to after validation)\n",
    "                    train_loss += loss.item()\n",
    "                \n",
    "                # Validation\n",
    "                model.eval()\n",
    "                val_loss = 0\n",
    "                with torch.no_grad():\n",
    "                    for batch_X, batch_y in val_loader:\n",
    "                        outputs = model(batch_X)\n",
    "                        val_loss += criterion(outputs, batch_y).item()\n",
    "                \n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "                          f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "                \n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    \n",
    "                # Update learning rate scheduler\n",
    "                scheduler.step(val_loss)\n",
    "                \n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_processed, _ = self.preprocess_data(X.values)\n",
    "        \n",
    "        predictions = []\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = model(X_processed)\n",
    "                predictions.append(pred.numpy())\n",
    "        \n",
    "        # Ensemble prediction (weighted median)\n",
    "        ensemble_pred = np.median(predictions, axis=0)\n",
    "        return ensemble_pred.flatten()\n",
    "\n",
    "def evaluate_ensemble_model(X, y, threshold=2.0):\n",
    "    # Split the data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train ensemble\n",
    "    ensemble = EnsemblePredictor(input_dim=X.shape[1])\n",
    "    ensemble.train(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions = ensemble.predict(X_train)\n",
    "    test_predictions = ensemble.predict(X_test)\n",
    "    \n",
    "    # Evaluate predictions\n",
    "    train_errors = np.abs(y_train - train_predictions)\n",
    "    test_errors = np.abs(y_test - test_predictions)\n",
    "    \n",
    "    train_within_threshold = train_errors <= threshold\n",
    "    test_within_threshold = test_errors <= threshold\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {np.mean(train_within_threshold):.4f}\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(train_errors):.2f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {np.mean(test_within_threshold):.4f}\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(test_errors):.2f}\")\n",
    "    \n",
    "    # Detailed error analysis\n",
    "    print(\"\\nDetailed Error Analysis (Test Set):\")\n",
    "    print(f\"Median Absolute Error: {np.median(test_errors):.2f}\")\n",
    "    print(f\"90th percentile of absolute error: {np.percentile(test_errors, 90):.2f}\")\n",
    "    \n",
    "    error_bins = pd.cut(test_errors, bins=[0, 1, 2, 3, 4, 5, float('inf')],\n",
    "                       labels=['0-1', '1-2', '2-3', '3-4', '4-5', '5+'])\n",
    "    error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n",
    "    \n",
    "    print(\"\\nError Distribution:\")\n",
    "    for bin_name, percentage in error_distribution.items():\n",
    "        print(f\"Error {bin_name} BMI points: {percentage*100:.1f}%\")\n",
    "    \n",
    "    return ensemble, test_errors\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    \n",
    "    # Train and evaluate the ensemble model\n",
    "    ensemble_model, errors = evaluate_ensemble_model(x, y, threshold=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
