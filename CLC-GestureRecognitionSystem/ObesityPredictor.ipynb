{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def preprocess_obesity_data(file_path):\n",
    "    # 1. Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    # 3. Simple data validation\n",
    "    # Remove any rows where height or weight are unreasonable\n",
    "    df = df[(df['Height'] > 1.4) & (df['Height'] < 2.2) &\n",
    "            (df['Weight'] > 40) & (df['Weight'] < 200)]\n",
    "    \n",
    "    # 4. Encode categorical variables using Label Encoder\n",
    "    categorical_cols = ['Gender', 'family_history', 'FAVC', 'CAEC', \n",
    "                       'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    # 5. Scale numerical variables (now without Weight)\n",
    "    numerical_cols = ['Age', 'Height', 'FCVC', 'NCP', \n",
    "                     'CH2O', 'FAF', 'TUE']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 6. Split features and target, dropping both BMI and Weight\n",
    "    X = df.drop(['BMI', 'Weight'], axis=1)\n",
    "    y = df['BMI']\n",
    "    \n",
    "    # 7. Print basic statistics\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nFeature names:\", list(X.columns))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Gender            0\n",
      "Age               0\n",
      "Height            0\n",
      "Weight            0\n",
      "family_history    0\n",
      "FAVC              0\n",
      "FCVC              0\n",
      "NCP               0\n",
      "CAEC              0\n",
      "SMOKE             0\n",
      "CH2O              0\n",
      "SCC               0\n",
      "FAF               0\n",
      "TUE               0\n",
      "CALC              0\n",
      "MTRANS            0\n",
      "BMI               0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (2105, 17)\n",
      "\n",
      "Feature names: ['Gender', 'Age', 'Height', 'family_history', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']\n"
     ]
    }
   ],
   "source": [
    "x,y =preprocess_obesity_data(\"ObesityPrediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       24.386526\n",
      "1       24.238227\n",
      "2       23.765432\n",
      "3       26.851852\n",
      "4       28.342381\n",
      "          ...    \n",
      "2106    44.901475\n",
      "2107    43.741923\n",
      "2108    43.543817\n",
      "2109    44.071535\n",
      "2110    44.144338\n",
      "Name: BMI, Length: 2105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Gender       Age    Height  family_history  FAVC      FCVC       NCP  \\\n",
      "0          0 -0.524137 -0.882009               1     0 -0.793282  0.402928   \n",
      "1          0 -0.524137 -1.956374               1     0  1.087496  0.402928   \n",
      "2          1 -0.209058  1.051847               1     0 -0.793282  0.402928   \n",
      "3          1  0.421101  1.051847               0     0  1.087496  0.402928   \n",
      "4          1 -0.366598  0.836974               0     0 -0.793282 -2.168920   \n",
      "...      ...       ...       ...             ...   ...       ...       ...   \n",
      "2106       0 -0.527786  0.092762               1     1  1.087496  0.402928   \n",
      "2107       0 -0.369285  0.499452               1     1  1.087496  0.402928   \n",
      "2108       0 -0.284041  0.538365               1     1  1.087496  0.402928   \n",
      "2109       0  0.005501  0.401319               1     1  1.087496  0.402928   \n",
      "2110       0 -0.104340  0.394723               1     1  1.087496  0.402928   \n",
      "\n",
      "      CAEC  SMOKE      CH2O  SCC       FAF       TUE  CALC  MTRANS  \n",
      "0        2      0 -0.014522    0 -1.187744  0.568638     3       3  \n",
      "1        2      1  1.616743    1  2.346544 -1.079475     2       3  \n",
      "2        2      0 -0.014522    0  1.168448  0.568638     1       3  \n",
      "3        2      0 -0.014522    0  1.168448 -1.079475     1       4  \n",
      "4        2      0 -0.014522    0 -1.187744 -1.079475     2       3  \n",
      "...    ...    ...       ...  ...       ...       ...   ...     ...  \n",
      "2106     2      0 -0.458000    0  0.787062  0.414123     2       3  \n",
      "2107     2      0 -0.006154    0  0.392542 -0.091810     2       3  \n",
      "2108     2      0  0.073881    0  0.478330 -0.014319     2       3  \n",
      "2109     2      0  1.375869    0  0.154233 -0.113623     2       3  \n",
      "2110     2      0  1.394097    0  0.021515  0.097504     2       3  \n",
      "\n",
      "[2105 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing feature engineering...\n",
      "Enhanced feature space shape: (2105, 46)\n",
      "Training ensemble model...\n",
      "Training neural network 1/5\n",
      "Network 1, Epoch [20/100], Loss: 1.3683\n",
      "Network 1, Epoch [40/100], Loss: 0.7799\n",
      "Network 1, Epoch [60/100], Loss: 0.7128\n",
      "Network 1, Epoch [80/100], Loss: 0.3877\n",
      "Network 1, Epoch [100/100], Loss: 0.3777\n",
      "Training neural network 2/5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class FeatureEngineering:\n",
    "    def __init__(self):\n",
    "        self.poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "        \n",
    "    def create_interaction_terms(self, X):\n",
    "        # Create polynomial and interaction features\n",
    "        numerical_cols = ['Age', 'Height', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE']\n",
    "        X_num = X[numerical_cols]\n",
    "        \n",
    "        # Generate polynomial features\n",
    "        poly_features = self.poly.fit_transform(X_num)\n",
    "        \n",
    "        # Generate correct feature names\n",
    "        feature_names = self.poly.get_feature_names_out(numerical_cols)\n",
    "        \n",
    "        # Create DataFrame with polynomial features\n",
    "        X_poly = pd.DataFrame(\n",
    "            poly_features,\n",
    "            columns=feature_names,\n",
    "            index=X.index\n",
    "        )\n",
    "        \n",
    "        # Create custom ratios\n",
    "        X_ratios = pd.DataFrame({\n",
    "            'height_age_ratio': X['Height'] / X['Age'],\n",
    "            'ch2o_faf_ratio': X['CH2O'] / (X['FAF'] + 1),  # Add 1 to avoid division by zero\n",
    "            'ncp_fcvc_ratio': X['NCP'] / (X['FCVC'] + 1)\n",
    "        }, index=X.index)\n",
    "        \n",
    "        # Drop the original features from X_poly since they're already in X\n",
    "        X_poly = X_poly.drop(columns=numerical_cols)\n",
    "        \n",
    "        # Combine all features\n",
    "        return pd.concat([X, X_poly, X_ratios], axis=1)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * 4, embed_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Self attention\n",
    "        attended, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attended)\n",
    "        \n",
    "        # Feed forward\n",
    "        fed_forward = self.feed_forward(x)\n",
    "        return self.norm2(x + fed_forward)\n",
    "\n",
    "class HybridBMIPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim=128):\n",
    "        super(HybridBMIPredictor, self).__init__()\n",
    "        \n",
    "        # Initial embedding\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(input_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads=4)\n",
    "            for _ in range(3)\n",
    "        ])\n",
    "        \n",
    "        # Parallel convolutional branch\n",
    "        self.conv_branch = nn.Sequential(\n",
    "            nn.Linear(input_dim, embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, embed_dim // 2)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(embed_dim + embed_dim // 2, embed_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(embed_dim // 2),\n",
    "            nn.Linear(embed_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transform input through transformer pathway\n",
    "        trans_input = self.embedding(x).unsqueeze(0)\n",
    "        for block in self.transformer_blocks:\n",
    "            trans_input = block(trans_input)\n",
    "        trans_output = trans_input.squeeze(0)\n",
    "        \n",
    "        # Process through convolutional pathway\n",
    "        conv_output = self.conv_branch(x)\n",
    "        \n",
    "        # Combine pathways\n",
    "        combined = torch.cat([trans_output, conv_output], dim=1)\n",
    "        return self.output(combined)\n",
    "\n",
    "class EnsemblePredictor:\n",
    "    def __init__(self, input_dim, ensemble_size=5):\n",
    "        self.neural_nets = [HybridBMIPredictor(input_dim) for _ in range(ensemble_size)]\n",
    "        self.random_forest = RandomForestRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=15,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2\n",
    "        )\n",
    "        \n",
    "    def train(self, X, y, epochs=100, batch_size=32):\n",
    "        # Convert data to tensors\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        y_tensor = torch.FloatTensor(y.values)\n",
    "        \n",
    "        # Train neural networks\n",
    "        criterion = nn.HuberLoss(delta=1.0)\n",
    "        \n",
    "        for i, net in enumerate(self.neural_nets):\n",
    "            print(f\"Training neural network {i+1}/{len(self.neural_nets)}\")\n",
    "            optimizer = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Training loop\n",
    "                net.train()\n",
    "                for j in range(0, len(X), batch_size):\n",
    "                    batch_X = X_tensor[j:j+batch_size]\n",
    "                    batch_y = y_tensor[j:j+batch_size]\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = net(batch_X)\n",
    "                    loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                if (epoch + 1) % 20 == 0:\n",
    "                    print(f\"Network {i+1}, Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "        # Train random forest\n",
    "        print(\"Training random forest...\")\n",
    "        self.random_forest.fit(X, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.FloatTensor(X.values)\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        nn_preds = []\n",
    "        for net in self.neural_nets:\n",
    "            net.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = net(X_tensor)\n",
    "                nn_preds.append(pred.numpy())\n",
    "        \n",
    "        rf_pred = self.random_forest.predict(X)\n",
    "        \n",
    "        # Weighted average of predictions\n",
    "        nn_weight = 0.7\n",
    "        rf_weight = 0.3\n",
    "        \n",
    "        final_pred = (\n",
    "            nn_weight * np.mean(nn_preds, axis=0) +\n",
    "            rf_weight * rf_pred.reshape(-1, 1)\n",
    "        )\n",
    "        \n",
    "        return final_pred.flatten()\n",
    "\n",
    "def train_enhanced_model(X, y):\n",
    "    # Feature engineering\n",
    "    print(\"Performing feature engineering...\")\n",
    "    feature_engineer = FeatureEngineering()\n",
    "    X_enhanced = feature_engineer.create_interaction_terms(X)\n",
    "    print(f\"Enhanced feature space shape: {X_enhanced.shape}\")\n",
    "    \n",
    "    # Create and train ensemble\n",
    "    print(\"Training ensemble model...\")\n",
    "    ensemble = EnsemblePredictor(input_dim=X_enhanced.shape[1])\n",
    "    ensemble.train(X_enhanced, y)\n",
    "    \n",
    "    return ensemble, feature_engineer\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model, feature_engineer = train_enhanced_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_bmi_predictions(model, feature_engineer, X, y, threshold=2.0):\n",
    "    \"\"\"\n",
    "    Generate comprehensive evaluation metrics for the BMI prediction model.\n",
    "    \"\"\"\n",
    "    # Enhance features\n",
    "    X_enhanced = feature_engineer.create_interaction_terms(X)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_enhanced)\n",
    "    \n",
    "    # Calculate absolute errors\n",
    "    errors = np.abs(y - predictions)\n",
    "    \n",
    "    # Create binary classifications based on threshold\n",
    "    y_true_binary = np.ones_like(y)  # Target is always 1 (within threshold)\n",
    "    y_pred_binary = (errors <= threshold).astype(int)\n",
    "    \n",
    "    # Generate classification report\n",
    "    print(\"\\nClassification Report (Within {} BMI points):\".format(threshold))\n",
    "    print(classification_report(y_true_binary, y_pred_binary, \n",
    "                              target_names=['Outside Threshold', 'Within Threshold']))\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    print(\"\\nDetailed Error Analysis:\")\n",
    "    print(f\"Mean Absolute Error: {np.mean(errors):.2f} BMI points\")\n",
    "    print(f\"Median Absolute Error: {np.median(errors):.2f} BMI points\")\n",
    "    print(f\"Standard Deviation of Error: {np.std(errors):.2f} BMI points\")\n",
    "    print(f\"90th percentile of absolute error: {np.percentile(errors, 90):.2f} BMI points\")\n",
    "    print(f\"95th percentile of absolute error: {np.percentile(errors, 95):.2f} BMI points\")\n",
    "    \n",
    "    # Error distribution\n",
    "    error_bins = pd.cut(errors, \n",
    "                       bins=[0, 0.5, 1, 1.5, 2, 2.5, 3, float('inf')],\n",
    "                       labels=['0-0.5', '0.5-1', '1-1.5', '1.5-2', '2-2.5', '2.5-3', '3+'])\n",
    "    \n",
    "    error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n",
    "    print(\"\\nError Distribution:\")\n",
    "    for bin_name, percentage in error_distribution.items():\n",
    "        print(f\"Error {bin_name} BMI points: {percentage*100:.1f}%\")\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(errors, bins=30)\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.xlabel('Absolute Error (BMI points)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold ({threshold} BMI)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot predicted vs actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y, predictions, alpha=0.5)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect Prediction')\n",
    "    plt.xlabel('Actual BMI')\n",
    "    plt.ylabel('Predicted BMI')\n",
    "    plt.title('Predicted vs Actual BMI')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'predictions': predictions,\n",
    "        'errors': errors,\n",
    "        'accuracy': accuracy_score(y_true_binary, y_pred_binary),\n",
    "        'error_distribution': error_distribution\n",
    "    }\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load and preprocess data\n",
    "    X, y = preprocess_obesity_data(\"Obesity prediction.csv\")\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    model, feature_engineer = train_enhanced_model(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    test_results = evaluate_bmi_predictions(model, feature_engineer, X_test, y_test, threshold=2.0)\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    print(\"\\nTraining Set Evaluation:\")\n",
    "    train_results = evaluate_bmi_predictions(model, feature_engineer, X_train, y_train, threshold=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
