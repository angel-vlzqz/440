{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def preprocess_obesity_data(file_path):\n",
    "    # 1. Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # 2. Check for missing values\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())\n",
    "    \n",
    "    # 3. Simple data validation\n",
    "    # Remove any rows where height or weight are unreasonable\n",
    "    df = df[(df['Height'] > 1.4) & (df['Height'] < 2.2) &\n",
    "            (df['Weight'] > 40) & (df['Weight'] < 200)]\n",
    "    \n",
    "    # 4. Encode categorical variables using Label Encoder\n",
    "    categorical_cols = ['Gender', 'family_history', 'FAVC', 'CAEC', \n",
    "                       'SMOKE', 'SCC', 'CALC', 'MTRANS']\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "    \n",
    "    # 5. Scale numerical variables (now without Weight)\n",
    "    numerical_cols = ['Age', 'Height', 'FCVC', 'NCP', \n",
    "                     'CH2O', 'FAF', 'TUE']\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    # 6. Split features and target, dropping both BMI and Weight\n",
    "    X = df.drop(['BMI', 'Weight'], axis=1)\n",
    "    y = df['BMI']\n",
    "    \n",
    "    # 7. Print basic statistics\n",
    "    print(\"\\nDataset shape:\", df.shape)\n",
    "    print(\"\\nFeature names:\", list(X.columns))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Gender            0\n",
      "Age               0\n",
      "Height            0\n",
      "Weight            0\n",
      "family_history    0\n",
      "FAVC              0\n",
      "FCVC              0\n",
      "NCP               0\n",
      "CAEC              0\n",
      "SMOKE             0\n",
      "CH2O              0\n",
      "SCC               0\n",
      "FAF               0\n",
      "TUE               0\n",
      "CALC              0\n",
      "MTRANS            0\n",
      "BMI               0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (2105, 17)\n",
      "\n",
      "Feature names: ['Gender', 'Age', 'Height', 'family_history', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']\n"
     ]
    }
   ],
   "source": [
    "x,y =preprocess_obesity_data(\"ObesityPrediction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       24.386526\n",
      "1       24.238227\n",
      "2       23.765432\n",
      "3       26.851852\n",
      "4       28.342381\n",
      "          ...    \n",
      "2106    44.901475\n",
      "2107    43.741923\n",
      "2108    43.543817\n",
      "2109    44.071535\n",
      "2110    44.144338\n",
      "Name: BMI, Length: 2105, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Gender       Age    Height  family_history  FAVC      FCVC       NCP  \\\n",
      "0          0 -0.524137 -0.882009               1     0 -0.793282  0.402928   \n",
      "1          0 -0.524137 -1.956374               1     0  1.087496  0.402928   \n",
      "2          1 -0.209058  1.051847               1     0 -0.793282  0.402928   \n",
      "3          1  0.421101  1.051847               0     0  1.087496  0.402928   \n",
      "4          1 -0.366598  0.836974               0     0 -0.793282 -2.168920   \n",
      "...      ...       ...       ...             ...   ...       ...       ...   \n",
      "2106       0 -0.527786  0.092762               1     1  1.087496  0.402928   \n",
      "2107       0 -0.369285  0.499452               1     1  1.087496  0.402928   \n",
      "2108       0 -0.284041  0.538365               1     1  1.087496  0.402928   \n",
      "2109       0  0.005501  0.401319               1     1  1.087496  0.402928   \n",
      "2110       0 -0.104340  0.394723               1     1  1.087496  0.402928   \n",
      "\n",
      "      CAEC  SMOKE      CH2O  SCC       FAF       TUE  CALC  MTRANS  \n",
      "0        2      0 -0.014522    0 -1.187744  0.568638     3       3  \n",
      "1        2      1  1.616743    1  2.346544 -1.079475     2       3  \n",
      "2        2      0 -0.014522    0  1.168448  0.568638     1       3  \n",
      "3        2      0 -0.014522    0  1.168448 -1.079475     1       4  \n",
      "4        2      0 -0.014522    0 -1.187744 -1.079475     2       3  \n",
      "...    ...    ...       ...  ...       ...       ...   ...     ...  \n",
      "2106     2      0 -0.458000    0  0.787062  0.414123     2       3  \n",
      "2107     2      0 -0.006154    0  0.392542 -0.091810     2       3  \n",
      "2108     2      0  0.073881    0  0.478330 -0.014319     2       3  \n",
      "2109     2      0  1.375869    0  0.154233 -0.113623     2       3  \n",
      "2110     2      0  1.394097    0  0.021515  0.097504     2       3  \n",
      "\n",
      "[2105 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      " Gender            0\n",
      "Age               0\n",
      "Height            0\n",
      "Weight            0\n",
      "family_history    0\n",
      "FAVC              0\n",
      "FCVC              0\n",
      "NCP               0\n",
      "CAEC              0\n",
      "SMOKE             0\n",
      "CH2O              0\n",
      "SCC               0\n",
      "FAF               0\n",
      "TUE               0\n",
      "CALC              0\n",
      "MTRANS            0\n",
      "BMI               0\n",
      "dtype: int64\n",
      "\n",
      "Dataset shape: (2105, 17)\n",
      "\n",
      "Feature names: ['Gender', 'Age', 'Height', 'family_history', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS']\n",
      "Epoch [10/100], Train Loss: 464.7842, Val Loss: 368.9088\n",
      "Epoch [20/100], Train Loss: 46.9658, Val Loss: 28.5821\n",
      "Epoch [30/100], Train Loss: 23.3892, Val Loss: 11.4793\n",
      "Epoch [40/100], Train Loss: 22.5239, Val Loss: 11.7526\n",
      "Early stopping at epoch 42\n",
      "\n",
      "Training Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.5784\n",
      "\n",
      "Test Set Performance:\n",
      "Accuracy (predictions within 2.0 BMI points): 0.5321\n",
      "\n",
      "Detailed Error Analysis (Test Set):\n",
      "Mean Absolute Error: 2.51 BMI points\n",
      "Median Absolute Error: 1.80 BMI points\n",
      "90th percentile of absolute error: 5.67 BMI points\n",
      "Percentage of predictions within 2.0 BMI points: 53.2%\n",
      "\n",
      "Error Distribution:\n",
      "Error 0-1 BMI points: 31.1%\n",
      "Error 1-2 BMI points: 22.1%\n",
      "Error 2-3 BMI points: 15.0%\n",
      "Error 3-4 BMI points: 11.6%\n",
      "Error 4-5 BMI points: 7.4%\n",
      "Error 5+ BMI points: 12.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ln/13_rf5392kz5pchj7wyd2nhm0000gn/T/ipykernel_68868/2405065893.py:155: FutureWarning: pandas.value_counts is deprecated and will be removed in a future version. Use pd.Series(obj).value_counts() instead.\n",
      "  error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class BMIDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X.values)\n",
    "        self.y = torch.FloatTensor(y.values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class BMINet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(BMINet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # First hidden layer\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # Second hidden layer\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            # Third hidden layer\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Dropout(0.1),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, threshold=2.0):\n",
    "    \"\"\"\n",
    "    Evaluate whether predictions are within threshold BMI points\n",
    "    Returns binary array where True means prediction was within threshold\n",
    "    \"\"\"\n",
    "    differences = np.abs(y_true - y_pred)\n",
    "    return differences <= threshold\n",
    "\n",
    "def train_and_evaluate_bmi_model(X, y, epochs=100, batch_size=32, threshold=2.0):\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = BMIDataset(X_train, y_train)\n",
    "    test_dataset = BMIDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = BMINet(X.shape[1])\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in test_loader:\n",
    "                outputs = model(batch_X)\n",
    "                val_loss += criterion(outputs, batch_y.unsqueeze(1)).item()\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(test_loader):.4f}')\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get all predictions\n",
    "        train_predictions = []\n",
    "        train_true = []\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            outputs = model(batch_X)\n",
    "            train_predictions.extend(outputs.numpy().flatten())\n",
    "            train_true.extend(batch_y.numpy().flatten())\n",
    "            \n",
    "        test_predictions = []\n",
    "        test_true = []\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            test_predictions.extend(outputs.numpy().flatten())\n",
    "            test_true.extend(batch_y.numpy().flatten())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    train_predictions = np.array(train_predictions)\n",
    "    train_true = np.array(train_true)\n",
    "    test_predictions = np.array(test_predictions)\n",
    "    test_true = np.array(test_true)\n",
    "    \n",
    "    # Evaluate predictions within threshold\n",
    "    train_within_threshold = evaluate_predictions(train_true, train_predictions, threshold)\n",
    "    test_within_threshold = evaluate_predictions(test_true, test_predictions, threshold)\n",
    "    \n",
    "    # Generate classification reports\n",
    "    print(\"\\nTraining Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {accuracy_score(train_within_threshold, np.ones_like(train_within_threshold)):.4f}\")\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"Accuracy (predictions within {threshold} BMI points): {accuracy_score(test_within_threshold, np.ones_like(test_within_threshold)):.4f}\")\n",
    "    \n",
    "    # Additional statistics\n",
    "    print(\"\\nDetailed Error Analysis (Test Set):\")\n",
    "    errors = np.abs(test_true - test_predictions)\n",
    "    print(f\"Mean Absolute Error: {np.mean(errors):.2f} BMI points\")\n",
    "    print(f\"Median Absolute Error: {np.median(errors):.2f} BMI points\")\n",
    "    print(f\"90th percentile of absolute error: {np.percentile(errors, 90):.2f} BMI points\")\n",
    "    print(f\"Percentage of predictions within {threshold} BMI points: {(100 * np.mean(test_within_threshold)):.1f}%\")\n",
    "    \n",
    "    # Create error distribution bins\n",
    "    error_bins = pd.cut(errors, bins=[0, 1, 2, 3, 4, 5, float('inf')], \n",
    "                       labels=['0-1', '1-2', '2-3', '3-4', '4-5', '5+'])\n",
    "    error_distribution = pd.value_counts(error_bins, normalize=True).sort_index()\n",
    "    print(\"\\nError Distribution:\")\n",
    "    for bin_name, percentage in error_distribution.items():\n",
    "        print(f\"Error {bin_name} BMI points: {percentage*100:.1f}%\")\n",
    "    \n",
    "    return model, errors\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming X and y are already preprocessed using your preprocess_obesity_data function\n",
    "    X, y = preprocess_obesity_data(\"ObesityPrediction.csv\")\n",
    "    \n",
    "    # Train and evaluate the model\n",
    "    model, errors = train_and_evaluate_bmi_model(X, y, threshold=2.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
